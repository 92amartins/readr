% Generated by roxygen2 (4.1.0): do not edit by hand
% Please edit documentation in R/tokenizer.R
\name{tokenize}
\alias{tokenize}
\title{Tokenize a file/string.}
\usage{
tokenize(file, tokenizer = tokenizer_csv(), n = NA_integer_)
}
\arguments{
\item{file}{Either a path to a file, a url, a connection, or literal data
   (either a single string or a raw vector). Connections and urls are saved
   to a temporary file before being read.

   Literal data is most useful for examples and tests. It must contain at
   least one new line to be recognised as data (instead of a path).}

\item{tokenizer}{A tokenizer specification.}

\item{n}{Optionally, maximum number of rows to tokenize.}
}
\description{
This turns a file into a list of tokens: each element of the list represents
one line. There are two special tokens: "[MISSING]" and "[EMPTY]".
}
\details{
Usually the tokenization is done purely in C++, and never exposed to R
(because that requires a copy). This function is useful for testing, or
when a file doesn't parse correctly and you want to see the underlying
tokens.
}
\examples{
tokenize("1,2\\n3,4,5\\n\\n6")

# Only tokenize first two lines
tokenize("1,2\\n3,4,5\\n\\n6", n = 2)
}
\keyword{internal}

