% Generated by roxygen2 (4.1.0): do not edit by hand
% Please edit documentation in R/tokenizer.R
\name{tokenize}
\alias{tokenize}
\title{Tokenize a file/string.}
\usage{
tokenize(path, text, tokenizer = csv_tokenizer(), n = NA_integer_)
}
\arguments{
\item{path}{Either a path to a file, or a connection. Reading directly
from a file is most efficient.}

\item{text}{A character or raw vector. If a character vector, only the
first element is used.}

\item{tokenizer}{A tokenizer specification.}

\item{n}{Optionally, maximum number of rows to tokenize.}
}
\description{
This turns a file into a list of tokens: each element of the list represents
one line. There are two special tokens: "[MISSING]" and "[EMPTY]".
}
\details{
Usually the tokenization is done purely in C++, and never exposed to R
(because that requires a copy). This function is useful for testing, or
when a file doesn't parse correctly and you want to see the underlying
tokens.
}
\examples{
tokenize(text = "1,2\\n3,4,5\\n\\n6")

# Only tokenize first two lines
tokenize(text = "1,2\\n3,4,5\\n\\n6", n = 2)
}
\keyword{internal}

